{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tiles3 import tiles, IHT  # This is the package for constructing the feature tilings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_selector(actions, values, epsilon):\n",
    "    \"\"\" An epsilon-greedy action selector.\n",
    "        Give list of actions, assumed values for each action,\n",
    "            and probability of NOT acting greedily.\n",
    "    \"\"\"\n",
    "    greedy = np.random.choice([1,0], p=[1-epsilon, epsilon])\n",
    "    if greedy:\n",
    "        return actions[3 - np.argmax(values[::-1]) - 1] # argmax in reverse to favor right action (not left)\n",
    "    else:\n",
    "        return np.random.choice(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mountain_car_td():\n",
    "    def __init__(self, n_episodes=100, rewards_list=list()):\n",
    "\n",
    "        self.n_episodes = n_episodes\n",
    "        self.n_timesteps = 1500  # Has to be this long to let it win by accident a few times, to start learning.\n",
    "        self.n_tilings = 10\n",
    "        self.tiling_dim = 10\n",
    "        self.max_tile_size = 1024 # ?? how to pick\n",
    "        self.iht = IHT(self.max_tile_size)\n",
    "\n",
    "        self.env = gym.make('MountainCar-v0')\n",
    "        self.env._max_episode_steps = self.n_timesteps\n",
    "        self.n_action = self.env.action_space.n\n",
    "        self.obs_highs = self.env.observation_space.high\n",
    "        self.obs_lows = self.env.observation_space.low\n",
    "        self.scales = [self.tiling_dim / (self.obs_highs[i] - self.obs_lows[i]) for i in range(len(self.obs_highs))]\n",
    "\n",
    "        # init weights\n",
    "        self.theta = np.zeros(self.max_tile_size *  self.n_action)\n",
    "        # basically a full set of weights per action, all concatenated together\n",
    "        \n",
    "        self.rewards_list = rewards_list # a continually growing list of rewards for sim runs\n",
    "\n",
    "    # Tiling helper function\n",
    "    # Because the tiles code only divides at integers, we must scale our observations to tiling_dim x tiling_dim space\n",
    "    # We also must shift to the section of theta corresponding to the considered action.\n",
    "    def mytiles(self, pt, a):\n",
    "        \"\"\" pt, point in observation space\n",
    "            a, an action\n",
    "            Returns featues for this state-action pair\n",
    "        \"\"\"\n",
    "        pt_ = np.array([p*self.scales[i] for i,p in enumerate(list(pt))])\n",
    "        features = list(np.array(tiles(self.iht, self.n_tilings, pt_)) + a * self.max_tile_size)\n",
    "        phi = np.zeros_like(self.theta)\n",
    "        phi[features] = 1\n",
    "        return phi\n",
    "\n",
    "    def run_sim(self, alpha0=1, render=False, verbose=False):\n",
    "        self.__init__(self.n_episodes, self.rewards_list)  # reset all weights and such\n",
    "        \n",
    "            # params\n",
    "        epsilon = .2 # (greedy)\n",
    "        gamma = .99\n",
    "        lam = .9\n",
    "        alpha = alpha0 / 10.\n",
    "        actions = range(self.env.action_space.n)\n",
    "\n",
    "        # Run the learning loop\n",
    "        rewards = []\n",
    "        \n",
    "        for i_episode in range(self.n_episodes):\n",
    "            epsilon *= .92\n",
    "            epsilon = max(epsilon, .04)\n",
    "            observation = self.env.reset()\n",
    "\n",
    "            # Choose an action espilon-greedily\n",
    "            phis = [self.mytiles(observation, a) for a in actions] # get features for observation\n",
    "            q_vals = [self.theta.dot(phi) for phi in phis]\n",
    "            action = action_selector(actions, q_vals, epsilon)\n",
    "            phi_sa = phis[action]\n",
    "            q_sa = q_vals[action]\n",
    "\n",
    "            # initialize e = 0\n",
    "            e = np.zeros_like(self.theta)\n",
    "\n",
    "            for t in range(self.n_timesteps):\n",
    "                if render: self.env.render()\n",
    "\n",
    "                # Take action, observe change, choose new a, get new features and q_sa_ and do update\n",
    "                observation, reward, done, info = self.env.step(action)\n",
    "\n",
    "                # Choose next action epsilon-greedily\n",
    "                phis = [self.mytiles(observation, a) for a in actions]\n",
    "                q_vals = [self.theta.dot(phi) for phi in phis]\n",
    "                action = action_selector(actions, q_vals, epsilon)\n",
    "                phi_sa_ = phis[action]\n",
    "                q_sa_ = q_vals[action]\n",
    "\n",
    "                # Perform updates\n",
    "                d = reward + gamma * q_sa_ - q_sa\n",
    "                e = gamma * lam * e + alpha * (1 - gamma * lam * e.dot(phi_sa)) * phi_sa\n",
    "                self.theta = self.theta + d * e + alpha * (q_sa - self.theta.dot(phi_sa)) * phi_sa\n",
    "\n",
    "\n",
    "                q_sa = q_sa_\n",
    "                phi_sa = phi_sa_\n",
    "\n",
    "\n",
    "                if done:\n",
    "                    rewards.append(t+1)\n",
    "                    if verbose: print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "                    break\n",
    "\n",
    "        self.env.close()\n",
    "        self.rewards_list.append(rewards)\n",
    "        return rewards\n",
    "    \n",
    "    def run_all_vals(self, n_runs=50):\n",
    "        # Return the average reward of the run for a range of alpha values, averaged over n_runs runs\n",
    "        average_rs = []\n",
    "        for a0 in np.linspace(.2, 2, 10):\n",
    "            print(a0)\n",
    "            for j in range(n_runs):\n",
    "                self.run_sim(alpha0 = a0)\n",
    "            r_average = -1 * np.array(self.rewards_list).mean(axis=1).mean()\n",
    "            average_rs.append(r_average)\n",
    "            self.rewards_list = []\n",
    "            \n",
    "        return average_rs\n",
    "                \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Episode finished after 1131 timesteps\n",
      "Episode finished after 713 timesteps\n",
      "Episode finished after 571 timesteps\n",
      "Episode finished after 592 timesteps\n",
      "Episode finished after 525 timesteps\n",
      "Episode finished after 245 timesteps\n",
      "Episode finished after 344 timesteps\n",
      "Episode finished after 203 timesteps\n",
      "Episode finished after 199 timesteps\n",
      "Episode finished after 236 timesteps\n",
      "Episode finished after 137 timesteps\n",
      "Episode finished after 209 timesteps\n",
      "Episode finished after 155 timesteps\n",
      "Episode finished after 212 timesteps\n",
      "Episode finished after 115 timesteps\n",
      "Episode finished after 119 timesteps\n",
      "Episode finished after 207 timesteps\n",
      "Episode finished after 201 timesteps\n",
      "Episode finished after 119 timesteps\n",
      "Episode finished after 142 timesteps\n",
      "Episode finished after 196 timesteps\n",
      "Episode finished after 223 timesteps\n",
      "Episode finished after 197 timesteps\n",
      "Episode finished after 187 timesteps\n",
      "Episode finished after 350 timesteps\n",
      "Episode finished after 201 timesteps\n",
      "Episode finished after 174 timesteps\n",
      "Episode finished after 172 timesteps\n",
      "Episode finished after 184 timesteps\n",
      "Episode finished after 141 timesteps\n",
      "Episode finished after 136 timesteps\n",
      "Episode finished after 154 timesteps\n",
      "Episode finished after 146 timesteps\n",
      "Episode finished after 144 timesteps\n",
      "Episode finished after 149 timesteps\n",
      "Episode finished after 151 timesteps\n",
      "Episode finished after 156 timesteps\n",
      "Episode finished after 159 timesteps\n",
      "Episode finished after 158 timesteps\n",
      "Episode finished after 156 timesteps\n",
      "Episode finished after 156 timesteps\n",
      "Episode finished after 144 timesteps\n",
      "Episode finished after 135 timesteps\n",
      "Episode finished after 146 timesteps\n",
      "Episode finished after 148 timesteps\n",
      "Episode finished after 143 timesteps\n",
      "Episode finished after 141 timesteps\n",
      "Episode finished after 134 timesteps\n",
      "Episode finished after 147 timesteps\n",
      "Episode finished after 144 timesteps\n",
      "Episode finished after 147 timesteps\n",
      "Episode finished after 146 timesteps\n",
      "Episode finished after 152 timesteps\n",
      "Episode finished after 150 timesteps\n",
      "Episode finished after 122 timesteps\n",
      "Episode finished after 149 timesteps\n",
      "Episode finished after 130 timesteps\n",
      "Episode finished after 142 timesteps\n",
      "Episode finished after 118 timesteps\n",
      "Episode finished after 143 timesteps\n",
      "Episode finished after 145 timesteps\n",
      "Episode finished after 146 timesteps\n",
      "Episode finished after 168 timesteps\n",
      "Episode finished after 140 timesteps\n",
      "Episode finished after 234 timesteps\n",
      "Episode finished after 254 timesteps\n",
      "Episode finished after 411 timesteps\n",
      "Episode finished after 228 timesteps\n",
      "Episode finished after 138 timesteps\n",
      "Episode finished after 218 timesteps\n"
     ]
    }
   ],
   "source": [
    "mc = mountain_car_td(n_episodes=70)\n",
    "rs = mc.run_sim(verbose=True, render=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot effect of learning rate on average rewards over first few episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will take some time to run, with their parameters.\n",
    "\n",
    "mc = mountain_car_td(n_episodes=10) # 20 episodes, they did.  Ours might need more, doesn't learn as fast.\n",
    "average_rs = mc.run_all_vals(n_runs=5) # 100 runs...  or fewer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(.2, 2, 5), last_rs, color='blue')\n",
    "plt.scatter(np.linspace(.2, 2, 5), last_rs, color='blue')\n",
    "plt.xlabel(r'$\\alpha_0$')\n",
    "plt.ylabel('return')\n",
    "plt.title('Effect of Learning Rate')\n",
    "# plt.savefig('mc_alpha_returns')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot rewards over time, averaged over some episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 50\n",
    "mc = mountain_car_td(n_episodes=30)\n",
    "for i in range(n_runs)\n",
    "    _ = mc.run_sim(verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = -1 * np.array(mc.rewards_list).mean(axis=0)\n",
    "plt.title('Mountain Car Rewards')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.plot(rs)\n",
    "plt.savefig('mc_rewards.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3 - np.argmax(values[::-1]) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(np.random.normal(0, .1, 150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,1],[2,2]])\n",
    "a.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
