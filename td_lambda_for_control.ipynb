{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tiles3 import tiles, IHT  # This is the package for constructing the feature tilings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_selector(actions, values, epsilon):\n",
    "    \"\"\" An epsilon-greedy action selector.\n",
    "        Give list of actions, assumed values for each action,\n",
    "            and probability of NOT acting greedily.\n",
    "    \"\"\"\n",
    "    greedy = np.random.choice([1,0], p=[1-epsilon, epsilon])\n",
    "    if greedy:\n",
    "        return actions[3 - np.argmax(values[::-1]) - 1] # argmax in reverse to favor right action (not left)\n",
    "    else:\n",
    "        return np.random.choice(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 100\n",
    "n_timesteps = 1500  # Has to be this long to let it win by accident a few times, to start learning.\n",
    "n_tilings = 10\n",
    "tiling_dim = 10\n",
    "max_tile_size = 1024 # ?? how to pick\n",
    "iht = IHT(max_tile_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "env._max_episode_steps = n_timesteps\n",
    "n_action = env.action_space.n\n",
    "obs_highs = env.observation_space.high\n",
    "obs_lows = env.observation_space.low\n",
    "scales = [tiling_dim / (obs_highs[i] - obs_lows[i]) for i in range(len(obs_highs))]\n",
    "\n",
    "\n",
    "# Tiling helper function\n",
    "# Because the tiles code only divides at integers, we must scale our observations to tiling_dim x tiling_dim space\n",
    "# We also must shift to the section of theta corresponding to the considered action.\n",
    "def mytiles(pt, a):\n",
    "    \"\"\" pt, point in observation space\n",
    "        a, an action\n",
    "        Returns featues for this state-action pair\n",
    "    \"\"\"\n",
    "    pt_ = np.array([p*scales[i] for i,p in enumerate(list(pt))])\n",
    "    features = list(np.array(tiles(iht, n_tilings, pt_)) + a * max_tile_size)\n",
    "    phi = np.zeros_like(theta)\n",
    "    phi[features] = 1\n",
    "    return phi\n",
    "\n",
    "# init weights\n",
    "theta = np.zeros(max_tile_size *  3) # basically a full set of weights per action, all concatenated together\n",
    "# theta = np.random.normal(0, .1, max_tile_size * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 1123 timesteps\n",
      "Episode finished after 1500 timesteps\n",
      "Episode finished after 913 timesteps\n",
      "Episode finished after 1114 timesteps\n",
      "Episode finished after 1050 timesteps\n",
      "Episode finished after 439 timesteps\n",
      "Episode finished after 743 timesteps\n",
      "Episode finished after 623 timesteps\n",
      "Episode finished after 327 timesteps\n",
      "Episode finished after 483 timesteps\n",
      "Episode finished after 759 timesteps\n",
      "Episode finished after 360 timesteps\n",
      "Episode finished after 330 timesteps\n",
      "Episode finished after 420 timesteps\n"
     ]
    }
   ],
   "source": [
    "# params\n",
    "epsilon = .6 # (greedy)\n",
    "gamma = .99\n",
    "lam = .9\n",
    "alpha = .1\n",
    "actions = range(env.action_space.n)\n",
    "\n",
    "# Run the learning loop\n",
    "for i_episode in range(n_episodes):\n",
    "    epsilon *= .95\n",
    "    epsilon = max(epsilon, .1)\n",
    "    observation = env.reset()\n",
    "    \n",
    "    # Choose an action espilon-greedily\n",
    "    phis = [mytiles(observation, a) for a in actions] # get features for observation\n",
    "    q_vals = [theta.dot(phi) for phi in phis]\n",
    "    action = action_selector(actions, q_vals, epsilon)\n",
    "    phi_sa = phis[action]\n",
    "    q_sa = q_vals[action]\n",
    "\n",
    "    # initialize e = 0\n",
    "    e = np.zeros_like(theta)\n",
    "    \n",
    "    for t in range(n_timesteps):\n",
    "        env.render()\n",
    "        \n",
    "        # Take action, observe change, choose new a, get new features and q_sa_ and do update\n",
    "        \n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Choose next action epsilon-greedily\n",
    "        phis = [mytiles(observation, a) for a in actions]\n",
    "        q_vals = [theta.dot(phi) for phi in phis]\n",
    "        action = action_selector(actions, q_vals, epsilon)\n",
    "        phi_sa_ = phis[action]\n",
    "        q_sa_ = q_vals[action]\n",
    "        \n",
    "#         if done: q_sa_ = 0 #????\n",
    "        \n",
    "        # Perform updates\n",
    "        d = reward + gamma * q_sa_ - q_sa\n",
    "        e = gamma * lam * e + alpha * (1 - gamma * lam * e.dot(phi_sa)) * phi_sa\n",
    "        theta = theta + d * e + alpha * (q_sa - theta.dot(phi_sa)) * phi_sa\n",
    "        \n",
    "        \n",
    "        q_sa = q_sa_\n",
    "        phi_sa = phi_sa_\n",
    "        \n",
    "    \n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ways to combine state features with actions\n",
    "\n",
    "# three separate theta vectors ? one per action? noooo\n",
    "# include as nother dimension in tiling?  Not clear how--actions are discrete\n",
    "# one theta, three times as long, each phi *= action*max_state_size)  <-- current implementation\n",
    "\n",
    "# Q: check: was it just the bigger gamma (.99) or the reverse argmax????  Try each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = [2,1,1]\n",
    "np.argmax(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3 - np.argmax(values[::-1]) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(np.random.normal(0, .1, 150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
