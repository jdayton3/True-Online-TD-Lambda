{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note: this random walk class can eventually be general, if we\n",
    "# find out how to algorithmically produce features given the desired\n",
    "# size of the random walk.  For now, I copied features out of the paper\n",
    "# and so it has to be size 6.\n",
    "class random_walk:\n",
    "    \n",
    "    def __init__(self, size, policy, gamma=.99):\n",
    "        \"\"\" size: int, size of random walk\n",
    "            policy: function, returns -1 (left) or 1 (right)\n",
    "            gamma: float between 0 and 1, discounting factor\n",
    "        \"\"\"\n",
    "        self.policy = policy\n",
    "        self.size = size\n",
    "        self.pos = int(size / 2) + 1  # start in the middle of the states\n",
    "        self.gamma = gamma\n",
    "        self.features = np.array([\n",
    "            [1,0,0,0,0],\n",
    "            [1/np.sqrt(2), 1/np.sqrt(2), 0,0,0],\n",
    "            [1/np.sqrt(3), 1/np.sqrt(3), 1/np.sqrt(3), 0, 0],\n",
    "            [0, 1/np.sqrt(3), 1/np.sqrt(3), 1/np.sqrt(3), 0],\n",
    "            [0, 0, 1/np.sqrt(3), 1/np.sqrt(3), 1/np.sqrt(3)],\n",
    "            [0,0,0,0,0]\n",
    "        ]) # these features are coming right out of the paper.  I did not figure out how he came up with them\n",
    "        \n",
    "        self.state = self.features[self.pos-1]  # pos describes the position of the walk.  state is the features at that position\n",
    "    \n",
    "    \n",
    "    def step(self):\n",
    "        \"\"\" Take a step according to the policy, and observe new state, reward, and terminality.\n",
    "            Returns: new state (features), reward, and a bool end which is True if terminal state, False otherwise\n",
    "        \"\"\"\n",
    "        self.pos += self.policy()\n",
    "#         if self.pos < 1: self.pos = 1       #   In the paper, state 1 was not terminal.  Right now it's terminal with reward 0\n",
    "        reward = 1 if self.pos == self.size else 0\n",
    "        \n",
    "        if self.pos == self.size or self.pos == 1:\n",
    "            end = True\n",
    "        else: end = False\n",
    "            \n",
    "        self.state = self.features[self.pos-1]\n",
    "    \n",
    "        return self.state, reward, end\n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\" Reset the random walk, at the end of an episode.\n",
    "        \"\"\"\n",
    "        self.__init__(self.size, self.policy)\n",
    "        \n",
    "    \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "class td_pred:\n",
    "    \"\"\" td_lambda for prediction (not control, that has to be a separate class)\n",
    "        with linear function approximation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, task, lam):\n",
    "        \"\"\" Task: hopefully this will be general enough that we can pass in a random walk class, mountain car task,\n",
    "                    or whatever else, and td_pred can use them all the same way.\n",
    "            lam: lambda\n",
    "        \"\"\"\n",
    "        self.task = task\n",
    "        self.weights = np.zeros_like(task.state)  # initialize weights like state features\n",
    "        self.lam = lam  # lambda in the algorithm\n",
    "        \n",
    "    \n",
    "    def learn(self, n, alpha):\n",
    "        \"\"\" Perform the td algorithm.  This is straight out of the paper\n",
    "            n: int, number of episodes\n",
    "            alpha: float from 0 to 1, step size\n",
    "            \n",
    "            returns: weights, state value estimates\n",
    "        \"\"\"\n",
    "        \n",
    "        gamma = self.task.gamma  \n",
    "        lam = self.lam\n",
    "        for _ in range(n):\n",
    "            e = np.zeros_like(self.task.state)\n",
    "            s = self.task.state\n",
    "            vs = self.weights.dot(self.task.state)\n",
    "            \n",
    "            while(1):\n",
    "                s_, r, t = self.task.step()\n",
    "                vs_ = self.weights.dot(s_)\n",
    "                d = r + self.task.gamma * vs_ - vs\n",
    "                e = gamma * lam * e + alpha * (1 - gamma * lam * e.dot(s)) * s\n",
    "                self.weights = self.weights + d * e + alpha * (vs - self.weights.dot(s)) * s\n",
    "                vs = vs_\n",
    "                s = s_\n",
    "                \n",
    "                if t:\n",
    "                    self.task.reset()\n",
    "                    break\n",
    "        \n",
    "        return self.weights, self.task.features.dot(self.weights)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a policy and instantiate random walk task\n",
    "p = [.5, .5]\n",
    "pol = lambda : np.random.choice([-1, 1], p=p)\n",
    "rw = random_walk(6, pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.39566899,  0.38965813,  0.40301077,  0.59082063,  0.59683149]),\n",
       " array([ 0.39566899,  0.55531014,  0.68608721,  0.79875806,  0.91836965,  0.        ]))"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A couple random walk demos\n",
    "\n",
    "td = td_pred(rw, .5)  # lambda .5\n",
    "td.learn(1000, .1)  # learning rate .1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.28448093,  0.25833193,  0.35337303,  0.5152536 ,  0.5414026 ]),\n",
       " array([ 0.28448093,  0.38382666,  0.51741317,  0.65064983,  0.81408075,  0.        ]))"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td = td_pred(rw, .5) # lambda .5\n",
    "td.learn(2000, .01)  # learning rate .01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.41994216,  0.39043767,  0.36121393,  0.55783837,  0.58734286]),\n",
       " array([ 0.41994216,  0.57302507,  0.67641997,  0.75603439,  0.86971765,  0.        ]))"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td = td_pred(rw, .2) # lambda .2\n",
    "td.learn(2000, .1)  # learning rate .1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.28378454,  0.2799094 ,  0.35632085,  0.53937073,  0.54324587]),\n",
       " array([ 0.28378454,  0.39859181,  0.53117079,  0.67873354,  0.83077092,  0.        ]))"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td = td_pred(rw, .2) # lambda .2\n",
    "td.learn(2000, .01)  # learning rate .01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
